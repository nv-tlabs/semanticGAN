<head>
</head>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
        margin: 0.4em;
    }

    p {
        margin: 0.2em;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        margin: 0;
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>semanticGAN</title>
        <meta property="og:title" content="semanticGAN" />
        <link rel="icon" href="./resources/nv_icon.png">
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:40px"> Semantic Segmentation with Generative Models (semanticGAN): Semi-Supervised Learning and Strong Out-of-Domain Generalization </span>
    </center>

    <br>
    <table align=center width=1000px>
    <tr>
        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><a href="https://scholar.google.ca/citations?user=8q2ISMIAAAAJ&hl=en">Daiqing Li</a><sup>1</sup></span>
            </center>
        </td>

        <td align=center width=100px>
                <center>
                <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=QYkscc4AAAAJ&hl=en">Junlin Yang</a><sup>1,3</sup></span>
                </center>
        </td>    
        <td align=center width=100px>
                <center>
                <span style="font-size:20px"><a href="https://scholar.google.de/citations?user=rFd-DiAAAAAJ&hl=de">Karsten Kreis</a><sup>1</sup></span>
                </center>
        </td>    
        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><a href="hhttps://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a><sup>5</sup></span>
            </center>
        </td>             
        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><a href="http://www.cs.toronto.edu/~fidler/">Sanja Fidler</a><sup>1,2,4</sup></span>
            </center>
        </td>
    </tr>
    </table>

    <br>
    <table align=center width=900>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><sup>1</sup>NVIDIA</span>
        </center>
        </td>
        <td align=center width=150px>
            <center>
            <span style="font-size:20px"><sup>2</sup>University of Toronto</span>
            </center>
        </td>
        <td align=center width=150px>
            <center>
            <span style="font-size:20px"><sup>3</sup>Yale University</span>
            </center>
        </td>
        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>4</sup>Vector Institute</span>
            </center>
        </td>

        <td align=center width=100px>
            <center>
            <span style="font-size:20px"><sup>5</sup>MIT</span>
            </center>
        </td>
     </tr>
    </table>
    <br>

    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px;color:red"> CVPR2021</span>
        </center>
        </td>
     </tr>
    </table>

    <br>
    <br>
    <table align=center width=900px>
        <tr>
            <td width="900px">
                <center>                                   
                <img src = "./resources/model.png" width="1000px" height="350px"></img>                        
                </center>
            </td>    
        </tr>


    </table>
    <table align=center width=900px></table>
        <tr>
            <td width=600px>
            <br>  
            <center>
                    <!--  -->
            </center>
            </td>
        </tr>
        <tr>
            <td width=600px>
                <br>
                <p align="justify" style="font-size: 18px">
                    Training deep networks with limited labeled data while achieving a strong generalization ability is key 
                    in the quest to reduce human annotation efforts. This is the goal of semisupervised learning, which exploits 
                    more widely available unlabeled data to complement small labeled data sets. In this paper, we propose a novel 
                    framework for discriminative pixel-level tasks using a generative model of both images and labels. Concretely, 
                    we learn a generative adversarial network that captures the joint image-label distribution and is trained 
                    efficiently using a large set of unlabeled images supplemented with only few labeled ones. We build our 
                    architecture on top of StyleGAN2, augmented with a label synthesis branch. Image labeling at test time is 
                    achieved by first embedding the target image into the joint latent space via an encoder network and test-time optimization, 
                    and then generating the label from the inferred embedding. We evaluate our approach in two important domains: 
                    medical image segmentation and part-based face segmentation. We demonstrate strong in-domain performance compared to 
                    several baselines, and are the first to showcase extreme out-of-domain generalization, such as transferring from CT to MRI 
                    in medical imaging, and photographs of real faces to paintings, sculptures, and even cartoons and animal faces.
                 </p>
            </td>
        </tr>
        <tr>
        </tr>
    </table>

    <br>
    <hr>
    <table align=center width=700>
        <center><h1>News</h1></center>
        <tr>
        <ul>
            <li style="font-size: 18px">[Jun 2021] Code release [<a href="https://github.com/nv-tlabs/semanticGAN_code">link</a>]!</li>
            <li style="font-size: 18px">[Jun 2021] Upload CVPR presentation video & poster</li>
            <li style="font-size: 18px">[Apr 2021] Project page is up</li>
        </ul>
        </tr>
    </table>
    <br>
    <hr>

    
    <hr>
        <table align=center width=900>
            <center><h1>Paper</h1></center>
            <tr>
                <td><a href="" target="_blank"><img style="height:180px; border: solid; border-radius:30px;" src="./resources/paper.png"/></a></td>
                <td><span style="font-size:18px">Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler
                <br><br>
                Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization
                <br><br>
                CVPR, 2021</font>
                <br>
                </td>
            </tr>
        </table>
        <br>
        <table align=center width=700px>
            <tr>
                <td>  
                    <span style="font-size:18px"><center>
                        <a href="https://arxiv.org/abs/2104.05833">[Paper]</a>
                    </center>
                </td>
                <td>   
                    <span style="font-size:18px"><center>
                        <a href="./resources/SemanticGAN_supp.pdf">[Supplement]</a>
                    </center>
                </td>
                <td>   
                    <span style="font-size:18px"><center>
                        <a href="./resources/cvpr21_poster_semanticGAN_update.pdf">[Poster]</a>
                    </center>
                </td>
                <td>
                    <span style="font-size:18px"><center>
                    <a href="resources/bibtex.txt">[Bibtex]</a>
                    </center>
                </td>

            </tr>
            <tr>
        
            </tr>
        </table>
        <br>
        
        <hr>
        <table align=center width=900px>	
            <center><h1>CVPR Presentation</h1></center>	

             <tr>
                <td>
                    &nbsp;
                    <!--you just need a space in a row-->
                </td>
             </tr>
             <tr>
                 <td colspan='4'>
                    <center>
                        <a href="./resources/semanticGAN_cvpr_presentation_update.mp4">
                            <video width="1080" controls muted>
                                <source src="./resources/semanticGAN_cvpr_presentation_update.mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>
                 </td>
                
             </tr>
             
             
            
            

        </table>
        <br>

        <hr>

        <hr>
        <table align=center width=900px>	
            <center><h1>Videos</h1></center>	
             <tr>
                 <th colspan='4'> 
                     <span style="font-size:22px">Face Parts Segmentation </span>
                 </th>
             </tr>
             <tr>
                <th colspan='4'
                    <center>
                    <span style="font-size:22px">
                        (Model trained on real human-face dataset CelebA only)
                    </span>
                    </center>
                </th>
                
             </tr>
             <tr>
                <td>
                    &nbsp;
                    <!--you just need a space in a row-->
                </td>
             </tr>
             <tr>
                 <td colspan='2'>
                    <center>
                        <a href="./resources/celeba-rand-diverse-loop-inter-merge.mp4">
                            <video width="512" controls autoplay loop muted>
                                <source src="./resources/celeba-rand-diverse-loop-inter-merge.mp4" type="video/mp4">
                            </video>
                        </a>
                    </center>
                 </td>
                 <td colspan='2'>
                    <center>
                        <a href="./resources/celeba-metface-inout-loop-inter-fix-noise-merge.mp4">
                            <video width="512" controls autoplay loop muted>
                                <source src="./resources/celeba-metface-inout-loop-inter-fix-noise-merge.mp4" type="video/mp4">
                            </video>
                        </a>
                        <br>
                    </center>
                 </td>
             </tr>
             
             <tr>
                <td colspan='2'> 
                    <center><span style="font-size:18px">Interpolation Between Celeba </span></center>
                </td>
                <td colspan='2'> 
                    <center><span style="font-size:18px">Interpolation Between Celeba and Out-of-domain Metface </span></center>
                </td>
             </tr>
             <tr>
                <td colspan='2'>
                   <center>
                       <a href="./resources/celeba-cariface-inout-loop-inter-fix-noise-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/celeba-cariface-inout-loop-inter-fix-noise-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                   </center>
                </td>
                <td colspan='2'>
                   <center>
                       <a href="./resources/celeba-extreme-inout-loop-inter-fix-noise-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/celeba-extreme-inout-loop-inter-fix-noise-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                       <br>
                   </center>
                </td>
            </tr>
            
            <tr>
               <td colspan='2'> 
                   <center><span style="font-size:18px">Interpolation Between Celeba and Out-of-domain Cariface</span></center>
               </td>
               <td colspan='2'> 
                   <center><span style="font-size:18px">Interpolation Between Celeba and Extreme Out-of-domain data</span></center>
               </td>
            </tr>
            <tr>
                <td>
                    &nbsp;
                    <!--you just need a space in a row-->
                </td>
             </tr>
            <tr>
                <th colspan='4'> 
                    <span style="font-size:22px">Chest X-ray Segmentation </span>
                </th>
            </tr>
            <tr>
               <td>
                   &nbsp;
                   <!--you just need a space in a row-->
               </td>
            </tr>
            <tr>
                <td colspan='2'>
                   <center>
                       <a href="./resources/cxr-rand-loop-inter-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/cxr-rand-loop-inter-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                   </center>
                </td>
                <td colspan='2'>
                   <center>
                       <a href="./resources/celeba-metface-inout-loop-inter-fix-noise-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/cxr-ood-opt-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                       <br>
                   </center>
                </td>
            </tr>
            <tr>
                <td colspan='2'> 
                    <center><span style="font-size:18px">Interpolation Between Random Chest X-ray Latent Codes</span></center>
                </td>
                <td colspan='2'> 
                    <center><span style="font-size:18px">Test-time Optimization Steps Visualization on Out-of-domain Data</span></center>
                </td>
             </tr>
             <tr>
                <td>
                    &nbsp;
                    <!--you just need a space in a row-->
                </td>
             </tr>
             <tr>
                <th colspan='4'> 
                    <span style="font-size:22px">CT-MRI Liver Segmentation </span>
                </th>
            </tr>
            <tr>
                <th colspan='4'
                    <center>
                    <span style="font-size:22px">
                        (Model trained on CT dataset only)
                    </span>
                    </center>
                </th>
                
             </tr>
            <tr>
               <td>
                   &nbsp;
                   <!--you just need a space in a row-->
               </td>
            </tr>
            <tr>
                <td colspan='2'>
                   <center>
                       <a href="./resources/lits-rand-loop-inter-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/lits-rand-loop-inter-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                   </center>
                </td>
                <td colspan='2'>
                   <center>
                       <a href="./resources/ctmri-lits-chaos-inout-loop-inter-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/ctmri-lits-chaos-inout-loop-inter-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                       <br>
                   </center>
                </td>
            </tr>
            <tr>
                <td colspan='2'> 
                    <center><span style="font-size:18px">Interpolation Between Random CT Latent Codes</span></center>
                </td>
                <td colspan='2'> 
                    <center><span style="font-size:18px">Interpolation Between Liver CT and CHAOS Liver T1-MRI</span></center>
                </td>
             </tr>
             <tr>
                <td>
                    &nbsp;
                    <!--you just need a space in a row-->
                </td>
             </tr>

             <!-- <tr>
                <th colspan='4'> 
                    <span style="font-size:22px">Skin Lesion Segmentation </span>
                </th>
            </tr>
            <tr>
               <td>
                   &nbsp;
                   
               </td>
            </tr>
            <tr>
                <td colspan='4'>
                   <center>
                       <a href="./resources/isic-rand-loop-inter-merge.mp4">
                           <video width="512" controls autoplay loop muted>
                               <source src="./resources/isic-rand-loop-inter-merge.mp4" type="video/mp4">
                           </video>
                       </a>
                   </center>
                </td>

            </tr>
            <tr>
                <td colspan='4'> 
                    <center><span style="font-size:18px">Interpolation Between Random Skin Lesion Latent Codes</span></center>
                </td>
            </tr> -->

        </table>
        <br>

        <hr>
        <center><h1>Face Parts Segmentation Optimization Steps</h1></center>
        <table align=center width=1000px>
            <tr>
                    <center>
                      <img class="round" style="height:600" src="./resources/face-parts-opt-steps.png"/></a>
                    </center>
          </tr>
          <tr>
            <td>
                &nbsp;
                <!--you just need a space in a row-->
            </td>
          </tr>
          <tr>
            <td>
                <span style="font-size:18px">
                    Image reconstructions and segmentation label predictions at different steps during the optimization process.
                    Step 0 corresponds to using the latent code predicted by the encoder without any further optimization. 
                    The model was trained on CelebA-Mask data. Hence, the first example corresponds to in-domain data, 
                    while the other two examples, from the MetFace dataset, are out-of-domain cases.
                </span>

            </td>
            
        </tr>
        </table>
        <br>

        <hr>
        <center><h1>Latent Space Interpolations</h1></center>
        <table align=center width=1000px>
            <tr>
                    <center>
                      <img class="round" style="height:700" src="./resources/inter-celeba.png"/></a>
                    </center>
            </tr>
            <tr>
                <center>
                  <img class="round" style="height:700" src="./resources/inter-metface.png"/></a>
                </center>
            </tr>
            <tr>
                <center>
                  <img class="round" style="height:700" src="./resources/inter-cartoon.png"/></a>
                </center>
            </tr>
          <tr>
            <td>
                &nbsp;
                <!--you just need a space in a row-->
            </td>
          </tr>
          <tr>
            <td>
                <span style="font-size:18px">
                    Linear interpolations between two random latent codes from celeba images to cother eleba, metface and cartoon images. 
                    We show both the interpolated images and their semantic segmentation labels. 
                    The results show that the generative model learnt a smooth latent space with meaningful images along the interpolation path. 
                    Furthermore, we observe consistency between images and predicted labels along the interpolation path.
                </span>

            </td>
            
        </tr>
        </table>

        <br>
        <hr>

        <center><h1> Quantitative Results</h1></center> <br>

        <table align=center width=900px>
            <tr>
                <td>

                    <span style="font-size:18px">
                        <b>Chest X-ray Lung Segmentation</b> Numbers are DICE scores. 
                        <a href='http://db.jsrt.or.jp/eng.php'>JSRT</a> is the in-domain dataset, 
                        on which we both train and evaluate. We also evaluate on additional out-of-domain datasets 
                        (<a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/'>NLM</a>, 
                        <a href='https://arxiv.org/abs/1803.01199'>NIH</a>, 
                        <a href='https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4256233/'>SZ</a>). 
                        Ours as well as the other semi-supervised methods use additional 108k unlabeled data samples.
                    </span>

                </td>
                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./resources/chest-xray-results.png"><img src = "./resources/chest-xray-results.png" width="1000px"></img></a><br>
                    </center>
                </td>
                
        </table>
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>

                    <span style="font-size:18px">
                        <b>Skin Lesion Segmentation</b> Numbers are JC index. <a href='https://challenge.isic-archive.com/'>ISIC</a>
                        is the in-domain dataset, on which we both train and evaluate. We also evaluate on additional out-of-domain datasets 
                        (<a href='https://pubmed.ncbi.nlm.nih.gov/24110966/'>PH2</a>, 
                        <a href='https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection'>IS</a>, 
                        <a href='https://uwaterloo.ca/vision-image-processing-lab/research-demos/skin-cancer-detection'>Quest</a>). 
                        Ours as well as the other semi-supervised methods use additional 33k unlabeled data samples.
                    </span>

                </td>
                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./resources/isic-results.png"><img src = "./resources/isic-results.png" width="1000px"></img></a><br>
                    </center>
                </td>
                
        </table>
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>

                    <span style="font-size:18px">
                        <b>CT-MRI Transfer Liver Segmentation</b> Numbers are DICE per patient. CT
                        is the in-domain data set on which we both train and evaluate. We also evaluate on additional unseen 
                        MRI T1-in and MRI T1-out from
                        <a href='https://chaos.grand-challenge.org/'>CHAOS</a> dataset. 
                        Ours as well as the semi-supervised methods use additional 70 CT volumes from the 
                        <a href='https://competitions.codalab.org/competitions/17094'>LITS2017</a> testing set as unlabeled data samples for training.
                    </span>

                </td>
                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./resources/ct-mri-results.png"><img src = "./resources/ct-mri-results.png" width="1000px"></img></a><br>
                    </center>
                </td>
                
        </table>
        <br>
        <hr>
        <br>
        <table align=center width=900px>
            <tr>
                <td>
                    <span style="font-size:18px">
                        <b>Face Part Segmentation</b> Numbers are mIoU. We train on CelebA and evaluate on CelebA (denoted as “In”)
                        as well as the MetFaces dataset. Train labels denotes the number of annotated examples used during training. 
                        Our model as well as the semi-supervised baselines additionally use 28k unlabeled CelebA data samples.
                    </span>

                </td>
                
            </tr>

            <tr>
                <td width=100px>
                    <center>
                        <a href="./resources/face-parts-results.png"><img src = "./resources/face-parts-results.png" width="600px"></img></a><br>
                    </center>
                </td>
                
        </table>
        <br>
        <br>
        <hr>
            
        

</body>
</html>